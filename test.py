import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os
import time

# ==================== é…ç½®åŒºåŸŸ ====================
# ä½ çš„åŸå§‹åŸºåº§æ¨¡å‹è·¯å¾„ (ä» Hugging Face ä¸‹è½½çš„)
BASE_MODEL_PATH = "./models/Qwen/Qwen2.5-3B-Instruct"

# ä½ å¾®è°ƒç”Ÿæˆçš„ LoRA é€‚é…å™¨è·¯å¾„ (æ ¹æ®ä½ çš„æˆªå›¾ï¼Œå°±æ˜¯è¿™ä¸ªæ–‡ä»¶å¤¹)
ADAPTER_PATH = "models/qwen_social_finetune_final"

# æ˜¾å­˜é…ç½® (ä¿æŒå’Œè®­ç»ƒæ—¶ä¸€è‡´çš„ FP16)
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# ==================== è§’è‰²äººè®¾å®šä¹‰ ====================
# æ³¨æ„ï¼šè¿™äº› SYSTEM PROMPT å¿…é¡»å’Œè®­ç»ƒæ•°æ®ä¸­çš„é£æ ¼ä¸€è‡´ï¼

ROLE_PROMPTS = {
    "é•¿è¾ˆ": (
        "ä½ æ˜¯ä¸€ä¸ªæƒ…å•†æé«˜çš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é•¿è¾ˆã€‘ã€‚"
        "è¯·ä¿æŒå°Šæ•¬ã€äº²åˆ‡çš„æ€åº¦ï¼Œå¹¶ä½¿ç”¨å¹½é»˜ã€æç¬‘æ„Ÿæ¥æ´»è·ƒæ°”æ°›ã€‚"
    ),
    "å¥³å‹": (
        "ä½ æ˜¯ä¸€ä¸ªé£è¶£å¹½é»˜çš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€å¥³å‹ã€‘ã€‚"
        "å¯¹è¯å……æ»¡ä¸­å›½å¼å¹½é»˜å´åˆä¸å¤±æš§æ˜§ï¼Œé€‚å½“åè½¬ã€‚å…¶ä»–æ—¶å€™è¦æœ‰ç”œç¾çš„æ„Ÿè§‰ã€‚"
    ),
    "å¯¼å¸ˆ": (
        "ä½ æ˜¯ä¸€ä¸ªç†å·¥ç§‘ç ”ç©¶ç”Ÿï¼Œæƒ…å•†å¾ˆé«˜ï¼Œè¯´è¯æœ‰åˆ†å¯¸ã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€å¯¼å¸ˆã€‘ã€‚"
        "æ•´ä½“é£æ ¼è¦ï¼šå°Šæ•¬ã€ä¸“ä¸šã€ç¤¼è²Œä¸ºä¸»ï¼ŒåŒæ—¶å¯ä»¥é€‚åº¦å¹½é»˜ã€æœºæ™ºã€‚"
    ),
    'é™Œç”Ÿäºº': (
        "ä½ æ˜¯ä¸€ä¸ªæœºæ™ºã€å¾—ä½“ã€æœ‰åˆ†å¯¸æ„Ÿçš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é™Œç”Ÿäººã€‘ã€‚"
        "ä¿æŒè½»æ¾ã€ç¤¼è²Œçš„æ€åº¦ï¼Œå¹¶ä½¿ç”¨é«˜æƒ…å•†å¹½é»˜æ¥åŒ–è§£å°´å°¬æˆ–æ‹‰è¿‘è·ç¦»ï¼Œ"
        "å¯¹äºå†’çŠ¯æˆ–å°´å°¬çš„é—®é¢˜è¦æœºæ™ºå›åº”ã€ä¿æŠ¤éšç§ï¼›å¯¹äºæ— å¿ƒçš„å°è¯¯ä¼šè¦ç”¨å¹½é»˜å±•ç°å–„æ„ã€‚"
        "å½“æ„Ÿè§‰æŠ•ç¼˜æ—¶ï¼Œå¯ä»¥é€‚åº¦åˆ†äº«ï¼Œç”¨å…±åŒè¯é¢˜å»ºç«‹è¿æ¥ã€‚"
        "å½“æ„Ÿè§‰ä¸å®‰å…¨æˆ–å¯¹æ–¹æ„å›¾ä¸å½“æ—¶ï¼Œç¤¼è²Œåœ°ç»“æŸå¯¹è¯å¹¶ç¦»å¼€ã€‚"
    ),
    'å¤«å¦»': (
        "ä½ æ˜¯ä¸€ä¸ªæƒ…å•†åœ¨çº¿ã€é£è¶£æš–å¿ƒçš„ä¼´ä¾£ã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é…å¶ã€‘ã€‚"
        "å¯¹è¯å……æ»¡ç”Ÿæ´»çƒŸç«æ°”ï¼Œå…¼å…·å¹½é»˜è°ƒä¾ƒä¸æ¸©æŸ”åŒ…å®¹ï¼Œå¶å°”äº’æ€¼å´ä¸ä¼¤äººã€‚"
        "å¯¹äºæ—¥å¸¸çäº‹å¤šæ¢ä½æ€è€ƒï¼Œå¯¹äºçŸ›ç›¾å·§å¦™åŒ–è§£ï¼Œå¯¹äºå…³å¿ƒåŠ å€å›åº”ï¼Œç”¨è½»æ¾è¯­æ°”ä¼ é€’çˆ±æ„ã€‚"
    )
}


def load_model_and_adapter():
    """åŠ è½½åŸºåº§æ¨¡å‹å’ŒLoRAé€‚é…å™¨"""
    print("--- 1. æ­£åœ¨åŠ è½½ Tokenizer ---")
    tokenizer = AutoTokenizer.from_pretrained(
        BASE_MODEL_PATH,
        trust_remote_code=True,
        padding_side="left"  # æ¨ç†æ—¶é€šå¸¸ä½¿ç”¨ left padding
    )
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("--- 2. æ­£åœ¨åŠ è½½ FP16 åŸºåº§æ¨¡å‹ ---")
    # æ³¨æ„: åŠ è½½æ—¶å¿…é¡»ä½¿ç”¨ FP16ï¼Œå› ä¸ºè®­ç»ƒæ—¶çš„æƒé‡ä¹Ÿæ˜¯åœ¨ FP16 æ¨¡å‹çš„å†…å­˜ä¸Šè®¡ç®—çš„
    base_model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",  # è‡ªåŠ¨åˆ†é…åˆ° GPU
        trust_remote_code=True
    )

    print(f"--- 3. æ­£åœ¨åŠ è½½ LoRA é€‚é…å™¨: {ADAPTER_PATH} ---")
    model = PeftModel.from_pretrained(
        base_model,
        ADAPTER_PATH,
        torch_dtype=torch.float16,
    )

    # å¯ç”¨è¯„ä¼°æ¨¡å¼ (ç¦ç”¨ Dropout ç­‰)
    model.eval()

    # æ‰“å°æ¨¡å‹ç»“æ„ï¼Œç¡®è®¤ LoRA æ³¨å…¥æˆåŠŸ
    print("\nâœ… æ¨¡å‹å’Œé€‚é…å™¨åŠ è½½æˆåŠŸï¼")
    return tokenizer, model


def generate_response(tokenizer, model, scenario, user_input):
    """æ ¹æ®åœºæ™¯å’Œè¾“å…¥ç”Ÿæˆå›å¤"""

    # 1. æ„é€  OpenAI æ ¼å¼çš„ Messages
    messages = [
        {"role": "system", "content": ROLE_PROMPTS[scenario]},
        {"role": "user", "content": user_input}
    ]

    # 2. åº”ç”¨ Qwen æ¨¡æ¿å¹¶ Tokenize
    input_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True  # å‘Šè¯‰æ¨¡å‹ï¼šæ¥ä¸‹æ¥è¯¥ä½ è¯´è¯äº†
    )

    input_ids = tokenizer(input_text, return_tensors="pt").input_ids.to(DEVICE)

    # 3. ç”Ÿæˆé…ç½®
    with torch.no_grad():
        output_ids = model.generate(
            input_ids=input_ids,
            max_new_tokens=512,
            do_sample=True,  # å¼€å¯é‡‡æ ·ï¼Œè®©å›ç­”æ›´å…·åˆ›é€ æ€§
            temperature=0.7,
            top_p=0.9,
            repetition_penalty=1.1  # é¿å…é‡å¤
        )

    # 4. è§£ç å¹¶æ¸…ç†è¾“å‡º
    output_text = tokenizer.decode(output_ids[0], skip_special_tokens=False)

    # Qwen çš„è¾“å‡ºéœ€è¦ç‰¹æ®Šå¤„ç†ï¼Œæå–åŠ©æ‰‹å›å¤éƒ¨åˆ†
    # æ‰¾åˆ°æœ€åä¸€ä¸ª Assistant çš„æ ‡è®°ï¼Œå¹¶æ¸…ç†åé¢çš„ EOS/IM_END æ ‡è®°
    if "<|im_start|>assistant" in output_text:
        assistant_start_index = output_text.rfind("<|im_start|>assistant")
        assistant_reply = output_text[assistant_start_index:].replace("<|im_start|>assistant\n", "").strip()
        assistant_reply = assistant_reply.replace("<|im_end|>", "").strip()

        # ç§»é™¤å¯èƒ½é‡å¤çš„ system prompt
        for prompt_text in ROLE_PROMPTS.values():
            if assistant_reply.startswith(prompt_text):
                assistant_reply = assistant_reply.replace(prompt_text, "").strip()

        return assistant_reply
    else:
        return output_text  # è¿”å›å®Œæ•´è¾“å‡ºä»¥ä¾¿è°ƒè¯•


if __name__ == "__main__":
    if not os.path.exists(ADAPTER_PATH):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°é€‚é…å™¨è·¯å¾„ {ADAPTER_PATH}")
        print("è¯·ç¡®è®¤ ADAPTER_PATH å’Œ BASE_MODEL_PATH é…ç½®æ˜¯å¦æ­£ç¡®ã€‚")
    else:
        # è®¡æ—¶å™¨
        start_time = time.time()

        tokenizer, model = load_model_and_adapter()

        load_time = time.time() - start_time
        print(f"\næ¨¡å‹æ€»åŠ è½½è€—æ—¶: {load_time:.2f} ç§’")

        print("\n" + "=" * 50)
        print("ğŸ¤– å¼€å§‹å¤šè§’è‰²é£æ ¼æµ‹è¯• ğŸ¤–")
        print("=" * 50)

        # --- æµ‹è¯•æ¡ˆä¾‹ 1: é•¿è¾ˆ ---
        scenario = "é•¿è¾ˆ"
        prompt = "å¹´çºªè½»è½»çš„ä¸æ‡‚å¾—å°Šè€çˆ±å¹¼å—ï¼Ÿ"
        print(f"--- åœºæ™¯: {scenario} ---")
        print(f"æé—®: {prompt}")
        response = generate_response(tokenizer, model, scenario, prompt)
        print(f"å›å¤: {response}\n")

        # --- æµ‹è¯•æ¡ˆä¾‹ 2: å¥³å‹ ---
        scenario = "å¥³å‹"
        prompt = "ä½ æ˜¯ä¸æ˜¯åœ¨æƒ³å‰å¥³å‹å‘¢ï¼Ÿ"
        print(f"--- åœºæ™¯: {scenario} ---")
        print(f"æé—®: {prompt}")
        response = generate_response(tokenizer, model, scenario, prompt)
        print(f"å›å¤: {response}\n")

        # --- æµ‹è¯•æ¡ˆä¾‹ 3: å¯¼å¸ˆ ---
        scenario = "å¯¼å¸ˆ"
        prompt = "ä½ å¹²ä»€ä¹ˆåƒçš„ï¼Œè¿™éƒ½ä¸ä¼šï¼Ÿ"
        print(f"--- åœºæ™¯: {scenario} ---")
        print(f"æé—®: {prompt}")
        response = generate_response(tokenizer, model, scenario, prompt)
        print(f"å›å¤: {response}\n")

        # --- æµ‹è¯•æ¡ˆä¾‹ 4: é™Œç”Ÿäºº ---
        scenario = "é™Œç”Ÿäºº"
        prompt = "å”‰ï¼Œä½ çœ‹é‚£ä¸ªäººç©¿å¾—å¥½å¥‡æ€ªå•Šã€‚"
        print(f"--- åœºæ™¯: {scenario} ---")
        print(f"æé—®: {prompt}")
        response = generate_response(tokenizer, model, scenario, prompt)
        print(f"å›å¤: {response}\n")

        # --- æµ‹è¯•æ¡ˆä¾‹ 5: å¤«å¦» ---
        scenario = "å¤«å¦»"
        prompt = "æˆ‘çœ‹ä½ è¿™å°±æ˜¯æ‡’ï¼Œè¿™ç‚¹å®¶åŠ¡éƒ½ä¸æƒ³åšï¼"
        print(f"--- åœºæ™¯: {scenario} ---")
        print(f"æé—®: {prompt}")
        response = generate_response(tokenizer, model, scenario, prompt)
        print(f"å›å¤: {response}\n")