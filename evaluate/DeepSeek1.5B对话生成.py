import os
import json
import pandas as pd
from tqdm import tqdm
import ollama

# ================= é…ç½®åŒºåŸŸ =================
# ç¡®ä¿è¿™ä¸ªæ¨¡å‹åå­—åœ¨ä½ çš„ cmd è¾“å…¥ 'ollama list' èƒ½çœ‹åˆ°
OLLAMA_MODEL_NAME = "gpt-oss:120b-cloud"

# æ–‡ä»¶è·¯å¾„é…ç½®
EVAL_DATA_DIR = "D:/program/ai_program/nlp_end_done/evaluate/data/"
OUTPUT_DIR = "D:/program/ai_program/nlp_end_done/evaluate/results5/"

SCENARIO_FILES = {
    "é•¿è¾ˆ": "elder_text.json",
    "å¥³å‹": "girl_text.json",
    "å¯¼å¸ˆ": "teacher_text.json",
    "é™Œç”Ÿäºº": "strange_text.json",
    "å¤«å¦»": "wife_text.json"
}

ROLE_PROMPTS = {
    "é•¿è¾ˆ": "ä½ æ˜¯ä¸€ä¸ªæƒ…å•†æé«˜çš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é•¿è¾ˆã€‘ã€‚è¯·ä¿æŒå°Šæ•¬ã€äº²åˆ‡çš„æ€åº¦ï¼Œå¹¶ä½¿ç”¨å¹½é»˜ã€æç¬‘æ„Ÿæ¥æ´»è·ƒæ°”æ°›ã€‚",
    "å¥³å‹": "ä½ æ˜¯ä¸€ä¸ªé£è¶£å¹½é»˜çš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€å¥³å‹ã€‘ã€‚å¯¹è¯å……æ»¡ä¸­å›½å¼å¹½é»˜å´åˆä¸å¤±æš§æ˜§ï¼Œé€‚å½“åè½¬ã€‚å…¶ä»–æ—¶å€™è¦æœ‰ç”œç¾çš„æ„Ÿè§‰ã€‚",
    "å¯¼å¸ˆ": "ä½ æ˜¯ä¸€ä¸ªç†å·¥ç§‘ç ”ç©¶ç”Ÿï¼Œæƒ…å•†å¾ˆé«˜ï¼Œè¯´è¯æœ‰åˆ†å¯¸ã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€å¯¼å¸ˆã€‘ã€‚æ•´ä½“é£æ ¼è¦ï¼šå°Šæ•¬ã€ä¸“ä¸šã€ç¤¼è²Œä¸ºä¸»ï¼ŒåŒæ—¶å¯ä»¥é€‚åº¦å¹½é»˜ã€æœºæ™ºã€‚",
    "é™Œç”Ÿäºº": "ä½ æ˜¯ä¸€ä¸ªæœºæ™ºã€å¾—ä½“ã€æœ‰åˆ†å¯¸æ„Ÿçš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é™Œç”Ÿäººã€‘ã€‚ä¿æŒè½»æ¾ã€ç¤¼è²Œçš„æ€åº¦ï¼Œå¹¶ä½¿ç”¨é«˜æƒ…å•†å¹½é»˜æ¥åŒ–è§£å°´å°¬æˆ–æ‹‰è¿‘è·ç¦»ã€‚",
    "å¤«å¦»": "ä½ æ˜¯ä¸€ä¸ªæƒ…å•†åœ¨çº¿ã€é£è¶£æš–å¿ƒçš„ä¼´ä¾£ã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é…å¶ã€‘ã€‚å¯¹è¯å……æ»¡ç”Ÿæ´»çƒŸç«æ°”ï¼Œå…¼å…·å¹½é»˜è°ƒä¾ƒä¸æ¸©æŸ”åŒ…å®¹ã€‚"
}

os.makedirs(OUTPUT_DIR, exist_ok=True)


class DeepChat:
    def __init__(self, model_name):
        self.model_name = model_name

    def chat(self, messages):
        """è°ƒç”¨ Ollama ç”Ÿæˆå›å¤"""
        try:
            response = ollama.chat(
                model=self.model_name,
                messages=messages,
                options={
                    "temperature": 0.7,
                    "top_p": 0.9,
                    "num_ctx": 4096
                }
            )
            return response["message"]["content"]
        except Exception as e:
            print(f"âš ï¸ Ollama è°ƒç”¨å‡ºé”™: {e}")
            return "Error: Generation Failed"


def format_history_for_excel(messages):
    text = ""
    for msg in messages:
        role = "AI" if msg['role'] == 'assistant' else "ç”¨æˆ·"
        if msg['role'] == 'system': continue
        text += f"[{role}]: {msg['content']}\n"
    return text.strip()


def main():
    bot = DeepChat(OLLAMA_MODEL_NAME)
    print(f"ğŸš€ å·²è¿æ¥ Ollama æ¨¡å‹: {OLLAMA_MODEL_NAME}")

    for role_name, filename in SCENARIO_FILES.items():
        file_path = os.path.join(EVAL_DATA_DIR, filename)
        if not os.path.exists(file_path):
            print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            continue

        print(f"\nğŸ¤– æ­£åœ¨é€è½®è¯„ä¼°åœºæ™¯ (Ollama): ã€{role_name}ã€‘...")
        current_system_prompt = ROLE_PROMPTS.get(role_name, "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        excel_data = []

        for session_idx, item in enumerate(tqdm(data, desc=f"å¤„ç† {role_name}")):
            messages = item['messages']

            for i in range(len(messages)):
                msg = messages[i]

                if msg['role'] == 'user' and (i + 1 < len(messages)) and messages[i + 1]['role'] == 'assistant':

                    raw_slice = messages[:i + 1]
                    input_msgs = [dict(m) for m in raw_slice]

                    if input_msgs[0]['role'] == 'system':
                        input_msgs[0]['content'] = current_system_prompt
                    else:
                        input_msgs.insert(0, {"role": "system", "content": current_system_prompt})

                    reference_answer = messages[i + 1]['content']
                    model_reply = bot.chat(input_msgs)
                    turn_index = (i + 1) // 2

                    excel_data.append({
                        "å¯¹è¯ID": session_idx + 1,
                        "è½®æ¬¡": f"ç¬¬ {turn_index} è½®",
                        "åœºæ™¯": role_name,
                        "å¯¹è¯å†å² (Context)": format_history_for_excel(input_msgs[:-1]),
                        "å½“å‰æé—®": msg['content'],
                        "ã€Ollamaæ¨¡å‹å›å¤ã€‘": model_reply,
                        "ã€å‚è€ƒå›å¤ã€‘": reference_answer,
                        "è¯„åˆ† (1-5)": ""
                    })

        df = pd.DataFrame(excel_data)

        # === ä¿®å¤ç‚¹ï¼šå¤„ç†æ–‡ä»¶åä¸­çš„å†’å·ï¼Œé˜²æ­¢ Windows æŠ¥é”™ ===
        safe_model_name = OLLAMA_MODEL_NAME.replace(":", "_")
        save_path = os.path.join(OUTPUT_DIR, f"{safe_model_name}_å¤šè½®è¯„ä¼°è¡¨_{role_name}.xlsx")

        df.to_excel(save_path, index=False)
        print(f"âœ… è¡¨æ ¼å·²ç”Ÿæˆ: {save_path}")


if __name__ == "__main__":
    main()