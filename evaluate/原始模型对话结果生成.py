import os
import json
import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer

# from peft import PeftModel # ä¸éœ€è¦åŠ è½½ LoRA é€‚é…å™¨äº†

# ================= é…ç½®åŒºåŸŸ (ç»å¯¹è·¯å¾„) =================
BASE_MODEL_PATH = "D:/program/ai_program/nlp_end_done/models/Qwen/Qwen2.5-3B-Instruct"
# ADAPTER_PATH = "D:/program/ai_program/nlp_end_done/models/qwen_social_finetune_final" # åŸå§‹æ¨¡å‹è¯„ä¼°ä¸éœ€è¦æ­¤è·¯å¾„
EVAL_DATA_DIR = "D:/program/ai_program/nlp_end_done/evaluate/data/"
OUTPUT_DIR = "D:/program/ai_program/nlp_end_done/evaluate/results2/"  # ä¿®æ”¹ä¸º result2 ç›®å½•

SCENARIO_FILES = {
    "é•¿è¾ˆ": "elder_text.json",
    "å¥³å‹": "girl_text.json",
    "å¯¼å¸ˆ": "teacher_text.json",
    "é™Œç”Ÿäºº": "strange_text.json",
    "å¤«å¦»": "wife_text.json"
}

ROLE_PROMPTS = {
    "é•¿è¾ˆ": "ä½ æ˜¯ä¸€ä¸ªæƒ…å•†æé«˜çš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é•¿è¾ˆã€‘ã€‚è¯·ä¿æŒå°Šæ•¬ã€äº²åˆ‡çš„æ€åº¦ï¼Œå¹¶ä½¿ç”¨å¹½é»˜ã€æç¬‘æ„Ÿæ¥æ´»è·ƒæ°”æ°›ã€‚",
    "å¥³å‹": "ä½ æ˜¯ä¸€ä¸ªé£è¶£å¹½é»˜çš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€å¥³å‹ã€‘ã€‚å¯¹è¯å……æ»¡ä¸­å›½å¼å¹½é»˜å´åˆä¸å¤±æš§æ˜§ï¼Œé€‚å½“åè½¬ã€‚å…¶ä»–æ—¶å€™è¦æœ‰ç”œç¾çš„æ„Ÿè§‰ã€‚",
    "å¯¼å¸ˆ": "ä½ æ˜¯ä¸€ä¸ªç†å·¥ç§‘ç ”ç©¶ç”Ÿï¼Œæƒ…å•†å¾ˆé«˜ï¼Œè¯´è¯æœ‰åˆ†å¯¸ã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€å¯¼å¸ˆã€‘ã€‚æ•´ä½“é£æ ¼è¦ï¼šå°Šæ•¬ã€ä¸“ä¸šã€ç¤¼è²Œä¸ºä¸»ï¼ŒåŒæ—¶å¯ä»¥é€‚åº¦å¹½é»˜ã€æœºæ™ºã€‚",
    "é™Œç”Ÿäºº": "ä½ æ˜¯ä¸€ä¸ªæœºæ™ºã€å¾—ä½“ã€æœ‰åˆ†å¯¸æ„Ÿçš„å·¥ç§‘å­¦ç”Ÿã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é™Œç”Ÿäººã€‘ã€‚ä¿æŒè½»æ¾ã€ç¤¼è²Œçš„æ€åº¦ï¼Œå¹¶ä½¿ç”¨é«˜æƒ…å•†å¹½é»˜æ¥åŒ–è§£å°´å°¬æˆ–æ‹‰è¿‘è·ç¦»ã€‚",
    "å¤«å¦»": "ä½ æ˜¯ä¸€ä¸ªæƒ…å•†åœ¨çº¿ã€é£è¶£æš–å¿ƒçš„ä¼´ä¾£ã€‚ä½ ç°åœ¨çš„å¯¹è¯å¯¹è±¡æ˜¯ä½ çš„ã€é…å¶ã€‘ã€‚å¯¹è¯å……æ»¡ç”Ÿæ´»çƒŸç«æ°”ï¼Œå…¼å…·å¹½é»˜è°ƒä¾ƒä¸æ¸©æŸ”åŒ…å®¹ã€‚"
}

os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_model():
    print("ğŸš€ æ­£åœ¨åŠ è½½åŸå§‹åŸºåº§æ¨¡å‹ (Base Model)...")
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True, padding_side="left")
    if tokenizer.pad_token is None: tokenizer.pad_token = tokenizer.eos_token

    # ç›´æ¥åŠ è½½åŸºåº§æ¨¡å‹ï¼Œä¸æŒ‚è½½ Adapter
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        torch_dtype=torch.float16,
        device_map="auto",
        trust_remote_code=True
    )

    model.eval()
    return tokenizer, model


def format_history_for_excel(messages):
    """æ ¼å¼åŒ–å†å²æ¶ˆæ¯ç”¨äºExcelå±•ç¤º"""
    text = ""
    for msg in messages:
        role = "AI" if msg['role'] == 'assistant' else "ç”¨æˆ·"
        if msg['role'] == 'system': continue
        text += f"[{role}]: {msg['content']}\n"
    return text.strip()


def generate_response(model, tokenizer, messages):
    input_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            repetition_penalty=1.1
        )
    return tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)


def main():
    tokenizer, model = load_model()

    for role_name, filename in SCENARIO_FILES.items():
        file_path = os.path.join(EVAL_DATA_DIR, filename)
        if not os.path.exists(file_path): continue

        print(f"\nğŸ¤– æ­£åœ¨é€è½®è¯„ä¼°åœºæ™¯ (åŸå§‹æ¨¡å‹): ã€{role_name}ã€‘...")
        current_system_prompt = ROLE_PROMPTS.get(role_name, "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹ã€‚")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        excel_data = []

        # éå†æ¯ä¸€ä¸ªå¯¹è¯ Session
        for session_idx, item in enumerate(tqdm(data, desc=f"å¤„ç† {role_name}")):
            messages = item['messages']

            # === æ ¸å¿ƒé€»è¾‘ï¼šéå†å¯¹è¯ä¸­çš„æ¯ä¸€è½® ===
            for i in range(len(messages)):
                msg = messages[i]

                # å¦‚æœå½“å‰æ˜¯ User å‘è¨€ï¼Œä¸”ä¸‹ä¸€æ¡æ˜¯ AI å‘è¨€ï¼Œè¿™å°±æ˜¯ä¸€ä¸ªæµ‹è¯•ç‚¹
                if msg['role'] == 'user' and (i + 1 < len(messages)) and messages[i + 1]['role'] == 'assistant':

                    # 1. æˆªå–åˆ°å½“å‰ User çš„å†å²ä½œä¸ºè¾“å…¥
                    input_msgs = messages[:i + 1]

                    # 2. å¼ºåˆ¶æ³¨å…¥ System Prompt
                    # æ³¨æ„ï¼šå¦‚æœä¸å¸Œæœ›åŸå§‹æ¨¡å‹å— System Prompt å½±å“å¤ªå¤§ï¼Œå¯ä»¥è€ƒè™‘æ³¨é‡Šæ‰è¿™éƒ¨åˆ†ï¼Œ
                    # ä½†ä¸ºäº†æ§åˆ¶å˜é‡æ³•å¯¹æ¯”å¾®è°ƒæ•ˆæœï¼Œå»ºè®®ä¿ç•™ System Promptã€‚
                    if input_msgs[0]['role'] == 'system':
                        input_msgs[0]['content'] = current_system_prompt
                    else:
                        input_msgs.insert(0, {"role": "system", "content": current_system_prompt})

                    # 3. æå–çœŸå€¼ (Ground Truth)
                    reference_answer = messages[i + 1]['content']

                    # 4. æ¨¡å‹ç”Ÿæˆ
                    model_reply = generate_response(model, tokenizer, input_msgs)

                    # 5. è®¡ç®—å½“å‰æ˜¯ç¬¬å‡ è½® (ç²—ç•¥è®¡ç®—)
                    turn_index = (i + 1) // 2

                    excel_data.append({
                        "å¯¹è¯ID": session_idx + 1,
                        "è½®æ¬¡": f"ç¬¬ {turn_index} è½®",
                        "åœºæ™¯": role_name,
                        "å¯¹è¯å†å² (Context)": format_history_for_excel(input_msgs[:-1]),
                        "å½“å‰æé—®": msg['content'],
                        "ã€åŸå§‹æ¨¡å‹å›å¤ã€‘": model_reply,  # è¡¨å¤´ç•¥ä½œåŒºåˆ†
                        "ã€å‚è€ƒå›å¤ã€‘": reference_answer,
                        "è¯„åˆ† (1-5)": ""
                    })

        # ä¿å­˜ Excel
        df = pd.DataFrame(excel_data)
        save_path = os.path.join(OUTPUT_DIR, f"åŸå§‹æ¨¡å‹_å¤šè½®è¯„ä¼°è¡¨_{role_name}.xlsx")
        df.to_excel(save_path, index=False)
        print(f"âœ… è¡¨æ ¼å·²ç”Ÿæˆ: {save_path}")


if __name__ == "__main__":
    main()