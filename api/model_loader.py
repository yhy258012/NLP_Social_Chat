import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# é…ç½®è·¯å¾„ (è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®)
BASE_MODEL_PATH = "../models/Qwen/Qwen2.5-3B-Instruct"
ADAPTER_PATH = "../models/qwen_social_finetune_final"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

class ModelService:
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super(ModelService, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
        return cls._instance

    def load_model(self):
        if self.model is not None:
            return

        print("ğŸš€ æ­£åœ¨åŠ è½½ Tokenizer...")
        self.tokenizer = AutoTokenizer.from_pretrained(
            BASE_MODEL_PATH,
            trust_remote_code=True,
            padding_side="left"
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token

        print("ğŸš€ æ­£åœ¨åŠ è½½åŸºåº§æ¨¡å‹ (FP16)...")
        base_model = AutoModelForCausalLM.from_pretrained(
            BASE_MODEL_PATH,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True
        )

        print(f"ğŸš€ æ­£åœ¨æ³¨å…¥ LoRA é€‚é…å™¨: {ADAPTER_PATH}...")
        self.model = PeftModel.from_pretrained(
            base_model,
            ADAPTER_PATH,
            torch_dtype=torch.float16,
        )
        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆï¼")

    def get_model(self):
        return self.tokenizer, self.model

# å…¨å±€å•ä¾‹
model_service = ModelService()